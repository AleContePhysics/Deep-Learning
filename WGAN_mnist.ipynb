{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd8bc64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7472168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=tf.keras.datasets.mnist.load_data()   #download MNIST dataset\n",
    "(train_images,train_labels),(test_images,test_labels)=data    #convert all the data into 4 tuples\n",
    "\n",
    "train_images=np.expand_dims(train_images,axis=-1)\n",
    "train_images=train_images.astype('float32')\n",
    "train_images=((train_images/255.0)*2)-1  #normalize the train images to the range [-1, 1]\n",
    "\n",
    "test_images=np.expand_dims(test_images,axis=-1)\n",
    "test_images=test_images.astype('float32')\n",
    "test_images=((test_images/255.0)*2)-1  #normalize the test images to the range [-1, 1]\n",
    "\n",
    "G=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=12544,input_shape=(100,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Reshape((7,7,256)),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=128,kernel_size=(3,3),padding=\"same\",strides=(2,2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=1,kernel_size=(3,3),activation=\"tanh\",padding=\"same\",strides=(2,2))\n",
    "])\n",
    "\n",
    "G.summary()\n",
    "\n",
    "D=tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\",strides=(2,2),input_shape=(28,28,1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"same\",strides=(2,2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "D.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "GP_WEIGHT=10.0 # Standard value for the Gradient Penalty weight\n",
    "bestfid=float('inf')\n",
    "save_path='best_generator.weights.h5' \n",
    "batch_size=256\n",
    "epochs=100\n",
    "latent_dim=100 \n",
    "D_loss_mean=[]\n",
    "G_loss_mean=[]\n",
    "D_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "G_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "dataset=tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_images)).batch(batch_size)\n",
    "\n",
    "@tf.function\n",
    "def Train_Step_WGANGP(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "    noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_images = G(noise, training=True)\n",
    "\n",
    "        real_output = D(real_images, training=True)\n",
    "        fake_output = D(fake_images, training=True)\n",
    "\n",
    "        # Loss WGAN\n",
    "        disc_wgan_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "        # Gradient Penalty\n",
    "        alpha = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0., maxval=1.)\n",
    "        interpolated_images = alpha * real_images + (1 - alpha) * fake_images\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_images)\n",
    "            pred = D(interpolated_images, training=True)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated_images])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "        disc_loss = disc_wgan_loss + GP_WEIGHT * gradient_penalty\n",
    "\n",
    "    D_Gradient = disc_tape.gradient(disc_loss, D.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_Gradient, D.trainable_variables))\n",
    "\n",
    "    # traning G\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        fake_images = G(noise, training=True)\n",
    "        fake_output = D(fake_images, training=True)\n",
    "\n",
    "        # Loss del Generatore\n",
    "        gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "    G_Gradient = gen_tape.gradient(gen_loss, G.trainable_variables)\n",
    "    G_optimizer.apply_gradients(zip(G_Gradient, G.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss\n",
    "for epoch in range(epochs):\n",
    "    epoch_d_loss = []\n",
    "    epoch_g_loss = []\n",
    "\n",
    "    for image_batch in dataset:\n",
    "        d_loss, g_loss = Train_Step_WGANGP(image_batch)\n",
    "        epoch_d_loss.append(d_loss.numpy())\n",
    "        epoch_g_loss.append(g_loss.numpy())\n",
    "\n",
    "    avg_d = np.mean(epoch_d_loss)\n",
    "    avg_g = np.mean(epoch_g_loss)\n",
    "\n",
    "    D_loss_mean.append(avg_d)\n",
    "    G_loss_mean.append(avg_g)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - D Loss: {avg_d:.4f}, G Loss: {avg_g:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of G and L mean loss for all 30 epochs\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,epochs+1),D_loss_mean,\"o-\",label=\"D Loss\",color=\"red\")\n",
    "plt.plot(range(1,epochs+1),G_loss_mean,\"o-\",label=\"G Loss\",color=\"blue\")\n",
    "plt.legend()\n",
    "plt.xticks(ticks=range(1,epochs+1))\n",
    "plt.title(\"Loss Comparation\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"D_loss value:\")\n",
    "for el in D_loss_mean:\n",
    "    print(el)\n",
    "print(\"G_loss value:\")\n",
    "for el in G_loss_mean:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c29b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 150\n",
    "\n",
    "def generate_and_display_images(model, num_images_to_generate):\n",
    "    noise = tf.random.normal([num_images_to_generate, latent_dim])\n",
    "    generated_images = model(noise, training=False)\n",
    "    generated_images = (generated_images + 1) / 2.0\n",
    "\n",
    "    grid_size = int(np.sqrt(num_images_to_generate))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_images_to_generate):\n",
    "        img = generated_images[i, :, :, 0]\n",
    "\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_and_display_images(G, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e278cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(model, real_images, fake_images):     #function for calculate the FID value\n",
    "    \n",
    "    act1 = model.predict(real_images)\n",
    "    act2 = model.predict(fake_images)\n",
    "\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "#image preparation for FID evaluation\n",
    "NUM_IMAGES_FOR_FID = 10000 \n",
    "\n",
    "real_images_subset = train_images[:NUM_IMAGES_FOR_FID]\n",
    "real_images_for_fid = (real_images_subset * 127.5 + 127.5).astype('uint8')\n",
    "real_images_resized = tf.image.resize(real_images_for_fid, [75, 75]).numpy()\n",
    "real_images_rgb = np.repeat(real_images_resized, 3, axis=-1)\n",
    "real_images_preprocessed = tf.keras.applications.inception_v3.preprocess_input(real_images_rgb)\n",
    "\n",
    "noise = tf.random.normal([NUM_IMAGES_FOR_FID, latent_dim])\n",
    "fake_images = G(noise, training=False)\n",
    "fake_images_for_fid = (fake_images * 127.5 + 127.5).numpy().astype('uint8')\n",
    "fake_images_resized = tf.image.resize(fake_images_for_fid, [75, 75]).numpy()\n",
    "fake_images_rgb = np.repeat(fake_images_resized, 3, axis=-1)\n",
    "fake_images_preprocessed = tf.keras.applications.inception_v3.preprocess_input(fake_images_rgb)\n",
    "\n",
    "\n",
    "inception_model = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    input_shape=(75, 75, 3)\n",
    ")\n",
    "\n",
    "fid_score = calculate_fid(inception_model, real_images_preprocessed, fake_images_preprocessed)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Final FID Score: {fid_score:.2f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa6b95",
   "metadata": {},
   "source": [
    "## S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43f4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47553a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569afcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
